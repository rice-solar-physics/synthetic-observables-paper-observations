%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                   Comparisons                               %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Classification Model}\label{sec:compare}

\begin{pycode}[manager_ml]
manager_ml = texfigure.Manager(
    pytex, './',
    python_dir='python',
    fig_dir='figures',
    data_dir='data',
    number=4,
)
from formatting import heating_palette, heating_cmap
from classify import prep_data, classify_ar
X, Y, X_observation, bad_pixels = prep_data(
    manager_ml.data_dir,
    channel_pairs,
    heating,
    correlation_threshold=correlation_threshold,
    rsquared_threshold=rsquared_threshold,
    scale_slope=False,
    scale_timelag=False,
    scale_correlation=False,
)
# Dummy metadata for creating maps
meta = Map(os.path.join(manager_ml.data_dir, 'observations', 'timelag_171_131.fits')).meta

# ML classification code here
rf_options = {
    'n_estimators': 250,
    'max_features': 'sqrt',
    'criterion': 'gini',
    'max_depth': 25,
    'min_samples_leaf': 1,
    'min_samples_split': 2,
    'bootstrap': True,
    'oob_score': True,
    'max_leaf_nodes': None,
    'min_impurity_decrease': 0,
    'n_jobs': -1,
}
frequency_maps = {}
probability_maps = {}
test_error = {}

# EM slope only
f_map, p_maps, _, err = classify_ar(rf_options, X[:,-1:], Y, X_observation[:,-1:], bad_pixels,)
frequency_maps['a'] = f_map
probability_maps['a'] = p_maps
test_error['a'] = err

# Timelags, cross-correlation only
f_map, p_maps, _, err = classify_ar(rf_options, X[:,:-1], Y, X_observation[:,:-1], bad_pixels)
frequency_maps['b'] = f_map
probability_maps['b'] = p_maps
test_error['b'] = err

# EM slope, timelags, cross-correlation
f_map, p_maps, clf, err = classify_ar(rf_options, X, Y, X_observation, bad_pixels)
frequency_maps['c'] = f_map
probability_maps['c'] = p_maps
test_error['c'] = err

# Calculate feature importances
importances = clf.feature_importances_
i_important = np.argsort(importances)[::-1]
std = np.std([t.feature_importances_ for t in clf.estimators_], axis=0)

# Top 10 features
f_map, p_maps, _, err = classify_ar(rf_options, X[:, i_important[:10]], Y,
                                    X_observation[:, i_important[:10]], bad_pixels)
frequency_maps['d'] = f_map
probability_maps['d'] = p_maps
test_error['d'] = err
\end{pycode}

To systematically assess whether the observations of NOAA 1158 are consistent with high-, intermediate-, or low-frequency heating as defined in \autoref{eq:heating_types}, we train a random forest classifier comprised of many decision tress on our predicted observables from \citetalias{barnes_understanding_2019} and use our trained model to classify each observed pixel in terms of the heating frequency. In the parlance of statistical learning, the heating frequency (low, intermediate, or high) is the \textit{class}, the emission measure slope, timelag, and cross-correlation are the \textit{features}, and the pixels are the \textit{samples}.

Following the explanation of \citet[chapter 8]{james_introduction_2013}, a decision tree recursively partitions the feature space of interest into a set of terminal nodes, or leaves, using a top-down, ``greedy'' approach called recursive binary splitting. At each node in the tree, a feature and an associated split point are chosen to maximize the number of observations of a single class in the resulting nodes. A common measure of the homogeneity or \textit{purity} of each node is the Gini index,
\begin{equation}\label{eq:gini-index}
    G_m = \sum_k \hat{p}_{mk} (1 - \hat{p}_{mk}),
\end{equation}
where $k$ indexes the class and $m$ indexes the node, and $\hat{p}_{mk}$ is the proportion of the observations at node $m$ that belong to class $k$. Note that as the purity of $m$ increases (i.e. $\hat{p}_{mk}\to0,1$), $G_m$ decreases ($G_m\to0$). Alternative measures of node purity may also be used \citep[see section 9.2.3 of][]{hastie_elements_2009}. For every resulting terminal node in the tree, the assigned class is determined by the most commonly occurring class of every observation at that node.

Decision trees are commonly used in classification problems because they are computationally efficient and relatively easy to interpret. Unlike many statistical learning techniques, decision trees do not assume any functional mapping between the inputs and outputs such that arbitrary, non-linear relatationships can be learned by the model. However, decision trees have two primary weaknesses: (1) they are known to have lower predictive accuracy than other more restrictive classification strategies and (2) they have high variance such that a single tree is not very robust to small changes in the training data \citep{james_introduction_2013}.

While individual trees may be error prone, random forest classifiers, first developed by \citet{breiman_random_2001}, provide an ensemble statistical learning method for combining many noisy, decorrelated decision trees in order to improve prediction accuracy and robustness. As in the bootstrap-aggregation, or ``bagging'', technique developed by \citet{breiman_bagging_1996}, each tree in the random forest is trained on only a subset of the total training data in order to reduce the variance of the model. Additionally, at each node in each tree, a random subset of the total features are considered as candidates for splitting in order to decrease the correlation between trees. This further reduces the variance and prevents a single feature from dominating the decision in every tree. Once each tree in the forest has been built using the training data, an unlabeled observation is classified by traversing each tree in the forest and taking the majority vote of the class at the terminal node of each tree. See chapter 15 of \citet{hastie_elements_2009} for a detailed discussion of random forests for both classification and regression.

\subsection{Data Preparation and Model Parameters}\label{sec:data-prep}

To build our classification model, we use the random forest classifier as implemented in the scikit-learn package for machine learning in Python \citep{pedregosa_scikit-learn_2011}. Using the predicted emission measure slopes, timelags, and maximum cross-correlations from \citetalias{barnes_understanding_2019}, we train a single random forest classifier composed of \py[manager_ml]|rf_options['n_estimators']| trees each with a maximum depth of \py[manager_ml]|rf_options['max_depth']|. At each node, $\left\lfloor\sqrt{31}\right\rfloor=\py|f'{int(np.sqrt(31)):.0f}'|$ possible split candidates are randomly selected from the $15\,\textup{timelags} + 15\,\textup{cross-correlations} + 1\,\textup{emission measure slope}=31$ total features.

Before training the model, we flatten the predicted emission measure slope, timelag, and cross-correlation maps from \citetalias{barnes_understanding_2019} for the high-, intermediate-, and low-frequency cases into an array of length $n_xn_y$, where $n_x$ and $n_y$ are the dimensions of the predicted images. As before, we mask pixels where $r^2<\py|rsquared_threshold|$ for the emission measure slope fit and where $\max\mathcal{C}_{AB}<\py|correlation_threshold|$ for the cross-correlation. If a pixel is masked in one frequency case, we mask it in all other frequencies to ensure that we have an equal number of high-, intermediate-, and low-frequency data points. We stack each flattened array column-wise in features and row-wise in heating frequency such that all of our predicted training data are encapsulated in a single data matrix $X$ of dimension $n\times p$. $p=31$ is the total number of features and $n=3n_xn_y - n_\textup{mask}=\py[manager_ml]|f'{X.shape[0]:.0f}'|$ is the total number of pixels for all heating frequencies minus those pixels which were masked in at least one feature of one frequency. The heating frequency label or class is numerically encoded as 0 (high), 1 (intermediate), or 2 (low) and similarly stacked to create a single response vector $Y$ of dimension $n\times1$. We apply a $2/3-1/3$ test-train split to $X$ and $Y$ such that approximately $1/3$ of the predicted data are reserved for model evaluation to ensure that our model has not overfit the data. This produces four separate matrices: $X_\textup{train},Y_\textup{train},X_\textup{test},Y_\textup{test}$. The data are not centered to a mean of 0 or scaled to unit standard deviation.

The same procedure as described above is applied to the observed emission measure slopes, timelags, and cross-correlations as shown in \autoref{fig:em-slopes}, \autoref{fig:timelags}, and \autoref{fig:correlations}, respectively. These results are flattened to a single data matrix $X^\prime$ of dimension $n^\prime\times p$, where $n^\prime=n_x^\prime n_y^\prime - n^\prime_\textup{mask}=\py[manager_ml]|f'{X_observation.shape[0]:.0f}'|$. The random forest model is trained on $X_\textup{train},Y_\textup{train}$ and model performance is evaluated on the ``unseen'' test set $X_\textup{test},Y_\textup{test}$. The trained model is then applied to $X^\prime$ in order to predict the heating frequency in each pixel, $Y^\prime$.

\subsection{Different Feature Combinations}\label{sec:feature-combos}

%Run classifier for EM, timelag+correlation, EM+timelag+correlation;
% describe how results are reshaped into map (briefly)
% compare results
% show heating frequency maps, probability maps for cases A-C

\begin{pycode}[manager_ml]
cases = ['a','b','c','d']
tab = {
    'Case': [c.upper() for c in cases],
    'Parameters': [r'$a$', r'$\tau_{AB},\mathcal{C}_{AB}$', r'$a,\tau_{AB},\mathcal{C}_{AB}$', r'Top 10 features from \autoref{tab:importance}'],
    'Error': [test_error[c] for c in cases],
    'High': [frequency_maps[c][frequency_maps[c] == 0].size/frequency_maps[c][~np.isnan(frequency_maps[c])].size for c in cases],
    'Intermediate': [frequency_maps[c][frequency_maps[c] == 1].size/frequency_maps[c][~np.isnan(frequency_maps[c])].size for c in cases],
    'Low': [frequency_maps[c][frequency_maps[c] == 2].size/frequency_maps[c][~np.isnan(frequency_maps[c])].size for c in cases],
}
caption = r'The four different combinations of emission measure slope, timelag, and maximum cross-correlation. The third column gives the misclassification error as evaluated on $X_\textup{test},Y_\textup{test}$. The fourth, fifth, and sixth columns show the percentage of pixels labeled as high-, intermediate-, and low-frequency heating, respectively.\label{tab:cases}'
formats = {
    'Error': '%.2f',
    'High': '%.3f',
    'Intermediate': '%.3f',
    'Low': '%.3f'
}
with io.StringIO() as f:
    ascii.write(tab, format='aastex', caption=caption, output=f, formats=formats)
    table = f.getvalue()
\end{pycode}
\py[manager_ml]|table|

We apply the train-test-predict procedure described above to all four cases listed in \autoref{tab:cases}. In case A, the random forest classifier is trained only on the emission measure slope, $a$, such that the $X$ and $X^\prime$ have dimensions $n\times1$ and $n^\prime\times1$, respectively. In case B, the classifier is trained on the timelags and maximum cross-correlations for all 15 channel pairs for a total of $p=30$ features while in case C, every feature (emission measure, 15 timelags, 15 maximum cross-correlations) is used such that $p=31$. We discuss case D in \autoref{sec:feature-importance}. The third column in \autoref{tab:cases} lists the misclassification error as evaluated on the test set, $X_\textup{test},Y_\textup{test}$ and the fourth, fifth, and sixth columns show the fraction of pixels classified as high-, intermediate-, and low-frequency.

\begin{pycode}[manager_ml]
fig = plt.figure(figsize=texfigure.figsize(
    pytex,
    scale=1 if is_onecolumn() else 2,
    height_ratio=4/3*0.96,
    figure_width_context='columnwidth'
))
axes = []
for j,c in enumerate(('a','b','c','d')):
    for i,h in enumerate(heating):
        m = GenericMap(probability_maps[c][h], meta)
        m = m.submap(SkyCoord(Tx=-410*u.arcsec,Ty=-325*u.arcsec,frame=m.coordinate_frame),
                     SkyCoord(Tx=-225*u.arcsec,Ty=-150*u.arcsec,frame=m.coordinate_frame))
        ax = fig.add_subplot(4, 3, 3*j+i+1, projection=m)
        axes.append(ax)
        im = m.plot(axes=ax, annotate=False, title=False, vmin=0, vmax=1, cmap='viridis',)
        ax.grid(alpha=0)
        lon,lat = ax.coords
        lon.set_ticks(number=4)
        lat.set_ticks(number=2)
        if i == 0 and j==3:
            lon.set_axislabel('Helioprojective Longitude',)
            lat.set_axislabel('Helioprojective Latitude', )
            lat.set_ticklabel(rotation='vertical')
        else:
            lat.set_ticklabel_visible(False)
            lon.set_ticklabel_visible(False)
        if i == 0:
            xtext,ytext = m.world_to_pixel(SkyCoord(-400*u.arcsec,-165*u.arcsec,frame=m.coordinate_frame))
            ax.text(int(xtext.value), int(ytext.value), f'{c.capitalize()}', color='k', fontsize=plt.rcParams['legend.fontsize'])
        if j == 0:
            xtext,ytext = m.world_to_pixel(SkyCoord(-230*u.arcsec,-315*u.arcsec,frame=m.coordinate_frame))
            ax.text(int(xtext.value), int(ytext.value),
                    h.split('_')[0].capitalize(),
                    horizontalalignment='right',
                    verticalalignment='bottom',
                    color='k', fontsize=plt.rcParams['legend.fontsize'])
plt.subplots_adjust(wspace=0.03,hspace=0.03)
cax = fig.add_axes([
    axes[0].get_position().get_points()[0,0],
    axes[0].get_position().get_points()[1,1]+0.0075,
    axes[-1].get_position().get_points()[1,0] - axes[0].get_position().get_points()[0,0],
    0.01
])
cbar = fig.colorbar(im, cax=cax, orientation='horizontal')
cbar.ax.xaxis.set_ticks_position('top')
### Save ###
fig_probability_maps = manager_ml.save_figure('probability-maps')
fig_probability_maps.caption = r'Classification probability for each pixel in the observed \AR{}. The rows denote the different cases in \autoref{tab:cases} and the columns correspond to the different heating frequency classes. If any of the 31 features is not valid in a particular pixel, the pixel is masked and colored white. Note that summing over all heating frequencies in each row gives 1 in every pixel.'
fig_probability_maps.figure_env_name = 'figure*'
fig_probability_maps.figure_width = r'\columnwidth' if is_onecolumn() else r'2\columnwidth'
fig_probability_maps.fig_str = fig_str
\end{pycode}
\py[manager_ml]|fig_probability_maps|

After computing the predicted heating frequency for each $X^\prime$, the resulting classifications, $Y^\prime$, are mapped back to the corresponding observed pixel locations to create a map of the heating frequency. \autoref{fig:probability-maps} shows the probability that each pixel corresponds to a particular heating frequency. The rows denote the different feature subsets as given in \autoref{tab:cases} and the columns correspond to the different heating frequency classes. The class probability, as computed by the scikit-learn package, in each pixel is the mean class probability of all trees in the random forest classifier. The class probability for an individual tree is the proportion of all training samples at the terminal node that belong to that class.

\begin{pycode}[manager_ml]
fig = plt.figure(figsize=texfigure.figsize(
    pytex,
    scale=1 if is_onecolumn() else 2,
    height_ratio=0.96,
    figure_width_context='columnwidth'
))
axes = []
for i,c in enumerate(('a','b','c','d')):
    m = GenericMap(frequency_maps[c],meta)
    m = m.submap(SkyCoord(Tx=-410*u.arcsec,Ty=-325*u.arcsec,frame=m.coordinate_frame),
                 SkyCoord(Tx=-225*u.arcsec,Ty=-150*u.arcsec,frame=m.coordinate_frame))
    ax = fig.add_subplot(2, 2, i+1, projection=m)
    axes.append(ax)
    im = m.plot(axes=ax, title=False,annotate=False, vmin=-0.5, vmax=2.5, cmap=heating_cmap())
    ax.grid(alpha=0)
    # Axes and ticks
    lon, lat = ax.coords
    if i == 2:
        lon.set_axislabel('Helioprojective Longitude',)
        lat.set_axislabel('Helioprojective Latitude',)
        lat.set_ticklabel(rotation='vertical')
    else:
        lon.set_ticklabel_visible(False)
        lat.set_ticklabel_visible(False)
    lon.set_ticks(number=4)
    lat.set_ticks(number=2)
    xtext,ytext = m.world_to_pixel(SkyCoord(-400*u.arcsec,-165*u.arcsec,frame=m.coordinate_frame))
    ax.text(int(xtext.value), int(ytext.value), f'{c.capitalize()}', color='k', fontsize=plt.rcParams['legend.fontsize'])
plt.subplots_adjust(wspace=0.03,hspace=0.03)
# Colorbar
cax = fig.add_axes([
    axes[0].get_position().get_points()[0,0],
    axes[0].get_position().get_points()[1,1] + 0.01,
    axes[-1].get_position().get_points()[1,0] - axes[0].get_position().get_points()[0,0],
    0.015
])
cbar = fig.colorbar(im, cax=cax,orientation='horizontal')
cbar.ax.xaxis.set_ticks_position('top')
cbar.set_ticks([-0.25,1,2.25])
cbar.ax.set_xticklabels([h.split('_')[0].capitalize() for h in heating],)
cbar.ax.tick_params(axis='x', which='both', length=0)
# Save
fig_frequency_maps = manager_ml.save_figure('frequency-maps')
fig_frequency_maps.caption = r'Predicted heating frequency classification in each pixel of NOAA 1158 for each of the cases in \autoref{tab:cases}. The classification is determined by which heating frequency class has the highest mean probability over all trees in the random forest. Each pixel is colored blue, orange, or green depending on whether the most likely heating frequency is high, intermediate, or low, respectively.'
fig_frequency_maps.figure_env_name = 'figure*'
fig_frequency_maps.figure_width = r'\columnwidth' if is_onecolumn() else r'2\columnwidth'
fig_frequency_maps.fig_str = fig_str
\end{pycode}
\py[manager_ml]|fig_frequency_maps|

\autoref{fig:frequency-maps} shows the heating frequency, or class, as predicted by the random forest classifier in each pixel of the observed \AR{} for all four cases in \autoref{tab:cases}. The predicted class is the one which has the highest mean probability as computed over all trees in the random forest. Each pixel is colored blue, orange, or green depending on whether the class with the highest mean probability is high-, intermediate-, or low-frequency, respectively.

We find that for each combination of features in \autoref{tab:cases}, high-frequency heating dominates at the center of the \AR{}. This result is consistent with \AR{} core observations of hot, steady emission \citep{warren_evidence_2010,warren_constraints_2011}, steep emission measure slopes \citep[e.g.][]{winebarger_using_2011,del_zanna_evolution_2015}, and lack of variability in the intensity \citep[e.g.][]{antiochos_constraints_2003} and the velocity \citep{brooks_flows_2009} near the loop footpoints. The frequency classification map for case A is as expected given the observed emission measure slope map in the left panel of \autoref{fig:em-slopes} and the well-separated distributions of emission measure slopes from the different heating frequencies as shown in the right panel of \autoref{fig:em-slopes}.

From the third column of \autoref{tab:cases}, we find that adding more features to the classifier significantly improves the accuracy as computed on the test data set. However, comparing frequency maps in cases A ($p=1$) and C ($p=31$), we find that the general pattern of heating frequency across the \AR{} is similar despite the large differences between the misclassification error in case A (\py[manager_ml]|f"{tab['Error'][0]:.2f}"|) and case C (\py[manager_ml]|f"{tab['Error'][2]:.2f}"|). Additionally, looking at the sixth column of \autoref{tab:cases} and the panels in the third column of \autoref{fig:probability-maps}, we find that adding the timelag and maximum cross-correlation features significantly decreases the number of pixels classified as low frequency. Training the classifier on only the emission measure slope versus all of the features has a relatively small impact on the fraction of pixels classified as intermediate frequency.

\subsection{Feature Importance}\label{sec:feature-importance}

In addition to the predicted heating frequency, $Y^\prime$, for a set of features, $X^\prime$, it is also useful to know which of the $p$ features is most important in deciding to which class each observation (pixel) belongs. One measure of the importance of each feature is the decrease in the Gini index,
\begin{equation}\label{eq:gini_gain}
    \Delta G_m = \frac{M_m}{M}\left( G_m - \frac{M_{m,R}}{M_m}G_{m,R} - \frac{M_{m,L}}{M_m}G_{m,L} \right),
\end{equation}
where $G_m$ is the Gini index as given by \autoref{eq:gini-index}, $M$ is the total number of samples in the tree, $M_m$ is the total number of samples at parent node $m$, $M_{m,R(L)}$ is the total number of samples at the right (left) child node, and $G_{m,R(L)}$ is the Gini index at the right (left) child node \citep{sandri_bias_2008}. The importance of a particular feature in the random forest classification is then determined by summing \autoref{eq:gini_gain} over all nodes which split on that feature for every tree and averaging over all trees \citep{breiman_classification_1984}.

Note that if $G_{m,R}=G_{m,L}=G_m$, $\Delta G_m=0$ because the split at node $m$ did not improve the discrimination between classes compared to the split at the previous node. However, if the purity of the left or right node increases such that $G_{m,R}$ or $G_{m,L}$ decreases, $\Delta G_m > 0$ because the split at node $m$ has added information to the classifier by preferentially sorting samples of a single class to either the left or right child node.

\begin{pycode}[manager_ml]
all_labels = np.array([r'$\tau_{{{},{}}}$'.format(*cp) for cp in channel_pairs] +
                      [r'$\mathcal{{C}}_{{{},{}}}$'.format(*cp) for cp in channel_pairs] + [r'$a$'])
tab2 = {
    'Feature': all_labels[i_important[:10]],
    'Importance': importances[i_important[:10]]/importances[i_important[0]],
    r'$\sigma$': std[i_important[:10]],
}
caption = r'Ten most important features as determined by the random forest classifier in case C. The second column shows the variable importance as computed by \autoref{eq:gini_gain} and the third column, $\sigma$, is the standard deviation of the feature importance over all trees in the random forest. The second column is normalized such that the most important feature is equal to 1.\label{tab:importance}'
formats = { 'Importance': '%.4f', r'$\sigma$': '%.4f' }
with io.StringIO() as f:
    ascii.write(tab2, format='aastex', caption=caption, output=f, formats=formats)
    table = f.getvalue()
\end{pycode}
\py[manager_ml]|table|

\autoref{tab:importance} shows the ten most important features from case C as determined by \autoref{eq:gini_gain} summed over all nodes in each tree and averaged over all trees. The importance in the second column is normalized such that the most important feature is equal to 1. According to \autoref{tab:importance}, the emission measure slope, $a$, has the most discriminating power in the random forest classifier. In particular, $a$ is more important than the second most important feature by over a factor of 2 and more important than the most important timelag feature by nearly an order of magnitude. Recall that the distributions for the different heating frequencies are reasonably well-separated in $a$ (right panel of \autoref{fig:em-slopes}), but strongly overlapping in $\tau_{AB}$ (see Figure 9 of \citetalias{barnes_understanding_2019}).

In case D as listed in the last row of \autoref{tab:cases}, only these ten features are used to train the random forest classifier and classify each observed pixel. The probability of each heating frequency for case D is shown in the last row of \autoref{fig:probability-maps} and the map of the most likely heating frequency in each pixel is shown in the bottom-right panel of \autoref{fig:frequency-maps}.We find that the probability maps in the last row of \autoref{fig:probability-maps} and the frequency map in the bottom right panel of \autoref{fig:frequency-maps} reveal approximately the same patterns of heating frequency across the \AR{} as the maps for case C in which all 31 features were included. Additionally, using less than $1/3$ of the total number of features, we acheive a misclassification error of \py[manager_ml]|f"{tab['Error'][3]:.2f}"|, comparable to case C.


