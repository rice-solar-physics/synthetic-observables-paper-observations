{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "We apply a Random Forest classifier to our timelag and correlation calculations over the whole AR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from scipy.interpolate import splev\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from sunpy.map import Map,GenericMap\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import dask\n",
    "import distributed\n",
    "from dask_ml.model_selection import RandomizedSearchCV\n",
    "from dask_ml.model_selection import train_test_split as dask_train_test_split\n",
    "\n",
    "from synthesizAR.instruments import InstrumentSDOAIA\n",
    "\n",
    "sys.path.append('../scripts/')\n",
    "from formatting import get_figsize,heating_palette,qualitative_palette,timelag_cmap,heating_cmap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=UserWarning,)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep and Import\n",
    "First, we need to import all of our timelag and correlation data, both modeled and observed, into data matrices, $X$, $Y$, and $X_{obs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "heating = ['high_frequency', 'intermediate_frequency', 'low_frequency',]\n",
    "labels = ['High', 'Intermediate', 'Low', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [94,131,171,193,211,335]\n",
    "channel_pairs = [(94,335),\n",
    "                 (94,171),\n",
    "                 (94,193),\n",
    "                 (94,131),\n",
    "                 (94,211),\n",
    "                 (335,131),\n",
    "                 (335,193),\n",
    "                 (335,211), \n",
    "                 (335,171),\n",
    "                 (211,131),\n",
    "                 (211,171),\n",
    "                 (211,193),\n",
    "                 (193,171),\n",
    "                 (193,131),\n",
    "                 (171,131),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_threshold = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_format = '/storage-home/w/wtb2/data/timelag_synthesis_v2/{}/nei/timelags/{}_{}_{}.fits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_timelag,X_correlation,Y = None,None,None\n",
    "for i,h in enumerate(heating):\n",
    "    _X_correlation,_X_timelag = None,None\n",
    "    # Find the indices where all channel pairs have an acceptable correlation\n",
    "    acceptable = np.all(np.stack([Map(file_format.format(h,'correlation',*cp)).data \n",
    "                                  for cp in channel_pairs], axis=2) > correlation_threshold, axis=2)\n",
    "    i_accept = np.where(acceptable)\n",
    "    for cp in channel_pairs:\n",
    "        # Read in timelags\n",
    "        # Only keep those timelags with sufficiently high correlations\n",
    "        tmp_tl = Map(file_format.format(h,'timelag',*cp)).data[i_accept].flatten()\n",
    "        # Stack along columns for different channel pairs\n",
    "        _X_timelag = tmp_tl.copy()[:, np.newaxis] if _X_timelag is None else np.hstack([_X_timelag, tmp_tl[:, np.newaxis]])\n",
    "        # Read in correlations\n",
    "        tmp_tl = Map(file_format.format(h,'correlation',*cp)).data[i_accept].flatten()\n",
    "        # Stack along columns for different channel pairs\n",
    "        _X_correlation = tmp_tl.copy()[:, np.newaxis] if _X_correlation is None else np.hstack([_X_correlation, tmp_tl[:, np.newaxis]])\n",
    "    # Stack along rows for different frequencies\n",
    "    X_timelag = _X_timelag.copy() if X_timelag is None else np.vstack([X_timelag, _X_timelag])\n",
    "    X_correlation = _X_correlation.copy() if X_correlation is None else np.vstack([X_correlation, _X_correlation])\n",
    "    # Get the frequency labels\n",
    "    _Y = np.array(_X_timelag.shape[0]*[labels[i]])\n",
    "    Y = _Y.copy() if Y is None else np.concatenate([Y, _Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(295956,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now encode each label with a numerical value in order to apply our classification method. We use the following scheme,\n",
    "\n",
    "* High frequency $\\to$ 0\n",
    "* Intermediate frequency $\\to$ 1\n",
    "* Low frequency $\\to$ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerically encode frequency\n",
    "le = LabelEncoder()\n",
    "le.fit(labels)\n",
    "Y_encoded = le.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['High', 'High', 'High', ..., 'Low', 'Low', 'Low'], dtype='<U12')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we scale the data matrix $\\mathbf{X}$ such that,\n",
    "\n",
    "$$\n",
    "\\mathbf{X} \\to \\frac{\\mathbf{X} - \\bar{\\mathbf{X}}}{\\sigma_{\\mathbf{X}}}\n",
    "$$\n",
    "\n",
    "where we apply this operation down each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scale(np.hstack([X_timelag,X_correlation],),axis=0, with_mean=True, with_std=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we perform a $2/3,1/3$ train/test split on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_encoded, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our training and test data matrices include both the timelags (columns 0-14) and the cross-correlation values (15-29)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import the observational data and again shape it into our data matrix $\\mathbf{X}_{obs}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_format = '/storage-home/w/wtb2/data/timelag_synthesis_v2/observational_data/timelags/{}_{}_{}.fits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the indices where all channel pairs have an acceptable correlation\n",
    "acceptable_correlation = np.all(np.stack([Map(file_format.format('correlation',*cp)).data\n",
    "                                          for cp in channel_pairs], axis=2) > correlation_threshold, axis=2)\n",
    "i_accept_obs = np.where(acceptable_correlation)\n",
    "# Stack maps\n",
    "X_observation_timelag,X_observation_correlation = None,None\n",
    "for cp in channel_pairs:\n",
    "    # Read in timelags\n",
    "    # Only keep those timelags with sufficiently high correlations\n",
    "    tmp_tl = Map(file_format.format('timelag', *cp)).data[i_accept_obs].flatten()\n",
    "    # Stack along columns for different channel pairs\n",
    "    X_observation_timelag = tmp_tl.copy()[:,np.newaxis] if X_observation_timelag is None else np.hstack([X_observation_timelag, tmp_tl[:, np.newaxis]])\n",
    "    # Read in correlation\n",
    "    tmp_tl = Map(file_format.format('correlation', *cp)).data[i_accept_obs].flatten()\n",
    "    # Stack along columns for different channel pairs\n",
    "    X_observation_correlation = tmp_tl.copy()[:,np.newaxis] if X_observation_correlation is None else np.hstack([X_observation_correlation, tmp_tl[:, np.newaxis]])\n",
    "# Scale data\n",
    "X_observation = scale(np.hstack([X_observation_timelag,X_observation_correlation]),axis=0, with_mean=True, with_std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train a Random Forest classifier on our model results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model\n",
    "To start off with, we'll choose a set of parameters that sound the best and we'll run the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_options = {\n",
    "    'n_estimators': 100,\n",
    "    'max_features': 'sqrt',\n",
    "    'criterion': 'entropy',\n",
    "    'max_depth': 25,\n",
    "    'min_samples_leaf': 1,\n",
    "    'bootstrap': True,\n",
    "    'oob_score': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(**rf_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train[:,:],Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test error = {1. - clf.score(X_test[:,:],Y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to do quite well. However, we should consider a range of hyper parameters before definitely settling on these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation via Random Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_parameters = {\n",
    "    'n_estimators': [10, 100, 200],\n",
    "    'max_features': ['sqrt', 1/3],\n",
    "    'max_depth': [10, 20, 50],\n",
    "    'min_samples_leaf': [1],\n",
    "    'criterion': ['entropy','gini'],\n",
    "    'bootstrap': [True,],\n",
    "    'oob_score': [True, False],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = distributed.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:32827\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>64</li>\n",
       "  <li><b>Cores: </b>64</li>\n",
       "  <li><b>Memory: </b>270.38 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:32827' processes=64 cores=64>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rcv = RandomizedSearchCV(rf, grid_parameters, n_iter=100,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dask, X_test_dask, Y_train_dask, Y_test_dask = dask_train_test_split(\n",
    "    dask.array.from_array(X_scaled,chunks=X_scaled.shape),\n",
    "    dask.array.from_array(Y_encoded,chunks=Y_encoded.shape),\n",
    "    test_size=0.33,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cache_cv=True, cv=None, error_score='raise',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [10, 100, 200], 'max_features': ['sqrt', 0.3333333333333333], 'max_depth': [10, 20, 50], 'min_samples_leaf': [1], 'criterion': ['entropy', 'gini'], 'bootstrap': [True], 'oob_score': [True, False]},\n",
       "          random_state=None, refit=True, return_train_score='warn',\n",
       "          scheduler=None, scoring=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rcv.fit(X_train_dask, Y_train_dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=50, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None,\n",
       "            oob_score=True, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rcv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then apply our trained model to our unlabeled observational data, computing both the label and the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_observation = clf.predict(X_observation[:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_observation_prob = clf.predict_proba(X_observation[:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last step, we want to transform these predicted data matrices back into maps of the active region. We do this for both the labeled frequencies as well as the probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = Map(file_format.format('timelag', *channel_pairs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_maps = {}\n",
    "for i,h in enumerate(labels):\n",
    "    tmp = dummy.data.copy() * np.nan\n",
    "    # Only those which we selected to classify originally are not NaN\n",
    "    i_select = np.where(le.inverse_transform(clf.classes_) == h)[0][0]\n",
    "    tmp[i_accept_obs] = Y_observation_prob[:, i_select]\n",
    "    probability_maps[h] = GenericMap(tmp, dummy.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = dummy.data.copy() * np.nan\n",
    "tmp[i_accept_obs] = Y_observation\n",
    "frequency_map = GenericMap(tmp, dummy.meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, make plots of both the frequency label as well as the probabilites of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=get_figsize(wf=0.85,))\n",
    "cax = fig.add_axes([0.275, 0.9, 0.475, 0.03])\n",
    "m = frequency_map.submap(SkyCoord(Tx=-440*u.arcsec,Ty=-380*u.arcsec,frame=m.coordinate_frame),\n",
    "                         SkyCoord(Tx=-185*u.arcsec,Ty=-125*u.arcsec,frame=m.coordinate_frame))\n",
    "ax = fig.gca(projection=m)\n",
    "im = m.plot(\n",
    "    axes=ax,\n",
    "    title=False,\n",
    "    annotate=False,\n",
    "    vmin=-0.5,\n",
    "    vmax=2.5,\n",
    "    cmap=heating_cmap()\n",
    ")\n",
    "ax.grid(alpha=0)\n",
    "# Axes and ticks\n",
    "lon, lat = ax.coords\n",
    "lon.set_ticklabel(fontsize=plt.rcParams['xtick.labelsize'],)\n",
    "lat.set_ticklabel(fontsize=plt.rcParams['ytick.labelsize'], rotation='vertical')\n",
    "lon.set_axislabel('Helioprojective Longitude [arcsec]', fontsize=plt.rcParams['axes.labelsize'])\n",
    "lat.set_axislabel('Helioprojective Latitude [arcsec]', fontsize=plt.rcParams['axes.labelsize'])\n",
    "lon.set_ticks(number=3)\n",
    "lat.set_ticks(number=3)\n",
    "# Colorbar\n",
    "cbar = fig.colorbar(im, cax=cax,orientation='horizontal')\n",
    "cbar.ax.xaxis.set_ticks_position('top')\n",
    "cbar.set_ticks([-0.25,1,2.25])\n",
    "cbar.ax.set_xticklabels(labels,)\n",
    "cbar.ax.tick_params(axis='x',which='both',length=0)\n",
    "fig.savefig('../paper/figures/frequency_map.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=get_figsize(wf=1.15,hf=0.375))\n",
    "cax = fig.add_axes([0.125, 0.85, 0.775, 0.03])\n",
    "for i,h in enumerate(labels[:3]):\n",
    "    m = probability_maps[h]\n",
    "    m = m.submap(SkyCoord(Tx=-440*u.arcsec,Ty=-380*u.arcsec,frame=m.coordinate_frame),\n",
    "                 SkyCoord(Tx=-185*u.arcsec,Ty=-125*u.arcsec,frame=m.coordinate_frame))\n",
    "    ax = fig.add_subplot(1,3,i+1, projection=m)\n",
    "    im = m.plot(axes=ax,\n",
    "                annotate=False,\n",
    "                title=False,\n",
    "                vmin=0,\n",
    "                vmax=1,\n",
    "                cmap='viridis',\n",
    "               )\n",
    "    ax.grid(alpha=0)\n",
    "    lon,lat = ax.coords\n",
    "    lon.set_ticks(number=3)\n",
    "    lat.set_ticks(number=3)\n",
    "    lon.set_ticklabel(fontsize=plt.rcParams['xtick.labelsize'])\n",
    "    if i > 0:\n",
    "        lat.set_ticklabel_visible(False)\n",
    "    else:\n",
    "        lat.set_ticklabel(fontsize=plt.rcParams['ytick.labelsize'], rotation='vertical')\n",
    "    if i == 0:\n",
    "        lon.set_axislabel('Helioprojective Longitude [arcsec]', fontsize=plt.rcParams['axes.labelsize'])\n",
    "        lat.set_axislabel('Helioprojective Latitude [arcsec]', fontsize=plt.rcParams['axes.labelsize'])\n",
    "    xtext,ytext = m.world_to_pixel(SkyCoord(-430*u.arcsec,-150*u.arcsec,frame=m.coordinate_frame))\n",
    "    xtext,ytext = int(xtext.value), int(ytext.value)\n",
    "    ax.text(xtext, ytext, h, color='k', fontsize=plt.rcParams['axes.labelsize'])\n",
    "cbar = fig.colorbar(im, cax=cax, orientation='horizontal')\n",
    "cbar.ax.xaxis.set_ticks_position('top')\n",
    "plt.subplots_adjust(wspace=0.02)\n",
    "fig.savefig('../paper/figures/probability_maps.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this combined case, let's plot the importance of each of the features in performing the classification in each pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "est_std = np.std([t.feature_importances_ for t in clf.estimators_], axis=0)\n",
    "fig = plt.figure(figsize=get_figsize(wf=2))\n",
    "ax = fig.gca()\n",
    "ax.bar(range(X_train.shape[1]), importances[indices], yerr=est_std[indices], color='k', align='center',alpha=0.5)\n",
    "#ax.set_ylim(0,0.2)\n",
    "ax.set_xticks(range(X_train.shape[1]),);\n",
    "all_labs = (\n",
    "    ['{}-{} $\\\\tau$'.format(*cp) for cp in channel_pairs] +\n",
    "    ['{}-{} $\\mathcal{{C}}$'.format(*cp) for cp in channel_pairs])\n",
    "ax.set_xticklabels(np.array(all_labs)[indices],fontsize=16);\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=-45, ha=\"left\", rotation_mode='anchor');\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_bounds(ax.get_ylim()[1], ax.get_ylim()[-2])\n",
    "ax.xaxis.set_tick_params(length=0)\n",
    "ax.set_ylabel('Feature Importance')\n",
    "ax.set_yticks(ax.get_yticks()[::2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:synthesized-timelags]",
   "language": "python",
   "name": "conda-env-synthesized-timelags-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
